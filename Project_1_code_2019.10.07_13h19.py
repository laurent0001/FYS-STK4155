# -*- coding: utf-8 -*-
"""Project_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11t0u48JqlQNIuVqfEGjXGuzBwVY4zgUo
"""

from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm
from matplotlib import scale
from matplotlib.ticker import LinearLocator, FormatStrFormatter
from random import random, seed
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import KFold
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.utils import resample
import time
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import scipy as scp

fig = plt.figure()
ax = fig.gca(projection="3d")

# Part A
print("Part A")
# Make data.
np.random.seed(1)


# Creating the design matrix
# Use np.c_ to create matrix using one array for each column
# Column of ones required

#print(DesignMatrix)
# np.meshgrid used after creation of design matrix to avoid 2 dimensional x & y


def FrankeFunction(x,y):
  term1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))
  term2 = 0.75*np.exp(-((9*x+1)**2)/49.0 - 0.1*(9*y+1))
  term3 = 0.5*np.exp(-(9*x-7)**2/4.0 - 0.25*((9*y-3)**2))
  term4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)
  return term1 + term2 + term3 + term4

def f_noise_franke(x, y):
  shape_z = np.shape(FrankeFunction(x,y))
  noise_z = 0.1*np.random.randn(shape_z[0], shape_z[1])
  return noise_z

# Creates design matrix for a polynomial up to degree of 'order'
def create_X(x, y, order):
    #number of combinations of powers in the polynomial
    n_terms = int((order+2)*(order+1)/2) 
    if hasattr(x, "__len__"):
        X = np.zeros((len(x), n_terms))
    else:
        X = np.zeros((1, n_terms))
    n = 0 #counter for which column we are at in the design matrix
    for j in range(0, order+1 ):
        for i in range(j + 1):
            X[:,n] = x**(j - i)*y**i
            n += 1
    return X
    #create_X(x,y,5)

#print(pd.DataFrame(f_noise_franke(x,y)))


#print(pd.DataFrame(z))
x = np.arange(0, 1, 0.025)
y = np.arange(0, 1, 0.025)
x, y = np.meshgrid(x,y)
z = FrankeFunction(x, y) + f_noise_franke(x, y)

# Plot the surface.
surf = ax.plot_surface(x, y, z, cmap=cm.coolwarm,linewidth=0, 
                       antialiased=False)

# Customize the z axis.
ax.set_zlim(-0.10, 1.40)
ax.zaxis.set_major_locator(LinearLocator(10))
ax.zaxis.set_major_formatter(FormatStrFormatter("%.02f"))

# Add a color bar which maps values to colors.
fig.colorbar(surf, shrink=0.5, aspect=5)
plt.show()

x = np.ravel(x)
y = np.ravel(y)
z = np.ravel(z)
X = np.c_[np.ones(len(x)), # We have to try different polynomial degrees for the report and discuss which ones are best
          x, y, 
          x*x, y*y, x*y,
          x*x*x, x*x*y, x*y*y, y*y*y,
          x*x*x*x, x*x*x*y, x*x*y*y, x*y*y*y, y*y*y*y,
          x*x*x*x*x, x*x*x*x*y, x*x*x*y*y, x*x*y*y*y, x*y*y*y*y, y*y*y*y*y]
DesignMatrix = pd.DataFrame(X)
DesignMatrix.columns = [
          '1',
          "x", "y", 
          "x^2", "xy", "y^2",
          "x^3", "x^2y", "xy^2", "y^3",
          "x^4", "x^3y", "x^2y^2", "xy^3", "y^4",
          "x^5", "x^4y", "x^3y^2", "x^2y^3", "xy^4", "y^5"]
X1 = np.c_[np.ones(x.shape[0]),
		x, y]
X2 = np.c_[np.ones(len(x)),
		x, y, 
		x*x, y*y, x*y]
X3 = np.c_[np.ones(x.shape[0]),
		x, y, 
		x*x, y*y, x*y,
		x*x*x, x*x*y, x*y*y, y*y*y]
X4 = np.c_[np.ones(x.shape[0]),
		x, y, 
		x*x, y*y, x*y,
		x*x*x, x*x*y, x*y*y, y*y*y,
		x*x*x*x, x*x*x*y, x*x*y*y, x*y*y*y, y*y*y*y]
X5 = np.c_[np.ones(x.shape[0]),
		x, y, 
		x*x, y*y, x*y,
		x*x*x, x*x*y, x*y*y, y*y*y,
		x*x*x*x, x*x*x*y, x*x*y*y, x*y*y*y, y*y*y*y,
		x*x*x*x*x, x*x*x*x*y, x*x*x*y*y, x*x*y*y*y, x*y*y*y*y, y*y*y*y*y]
X6 = np.c_[np.ones(x.shape[0]),
		x, y, 
		x*x, y*y, x*y,
		x*x*x, x*x*y, x*y*y, y*y*y,
		x*x*x*x, x*x*x*y, x*x*y*y, x*y*y*y, y*y*y*y,
		x*x*x*x*x, x*x*x*x*y, x*x*x*y*y, x*x*y*y*y, x*y*y*y*y, y*y*y*y*y,
		x*x*x*x*x*x, x*x*x*x*x*y, x*x*x*x*y*y, x*x*x*y*y*y, x*x*y*y*y*y, x*y*y*y*y*y, y*y*y*y*y*y]
X7 = np.c_[np.ones(x.shape[0]),
		x, y, 
		x*x, y*y, x*y,
		x*x*x, x*x*y, x*y*y, y*y*y,
		x*x*x*x, x*x*x*y, x*x*y*y, x*y*y*y, y*y*y*y,
		x*x*x*x*x, x*x*x*x*y, x*x*x*y*y, x*x*y*y*y, x*y*y*y*y, y*y*y*y*y,
		x*x*x*x*x*x, x*x*x*x*x*y, x*x*x*x*y*y, x*x*x*y*y*y, x*x*y*y*y*y, x*y*y*y*y*y, y*y*y*y*y*y,
		x*x*x*x*x*x*x, x*x*x*x*x*x*y, x*x*x*x*x*y*y, x*x*x*x*y*y*y, x*x*x*y*y*y*y, x*x*y*y*y*y*y, x*y*y*y*y*y*y, y*y*y*y*y*y*y]
X8 = np.c_[np.ones(x.shape[0]),
		x, y, 
		x*x, y*y, x*y,
		x*x*x, x*x*y, x*y*y, y*y*y,
		x*x*x*x, x*x*x*y, x*x*y*y, x*y*y*y, y*y*y*y,
		x*x*x*x*x, x*x*x*x*y, x*x*x*y*y, x*x*y*y*y, x*y*y*y*y, y*y*y*y*y,
		x*x*x*x*x*x, x*x*x*x*x*y, x*x*x*x*y*y, x*x*x*y*y*y, x*x*y*y*y*y, x*y*y*y*y*y, y*y*y*y*y*y,
		x*x*x*x*x*x*x, x*x*x*x*x*x*y, x*x*x*x*x*y*y, x*x*x*x*y*y*y, x*x*x*y*y*y*y, x*x*y*y*y*y*y, x*y*y*y*y*y*y, y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*y, x*x*x*x*x*x*y*y, x*x*x*x*x*y*y*y, x*x*x*x*y*y*y*y, x*x*x*y*y*y*y*y, x*x*y*y*y*y*y*y, x*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y]
X9 = np.c_[np.ones(x.shape[0]),
		x, y, 
		x*x, y*y, x*y,
		x*x*x, x*x*y, x*y*y, y*y*y,
		x*x*x*x, x*x*x*y, x*x*y*y, x*y*y*y, y*y*y*y,
		x*x*x*x*x, x*x*x*x*y, x*x*x*y*y, x*x*y*y*y, x*y*y*y*y, y*y*y*y*y,
		x*x*x*x*x*x, x*x*x*x*x*y, x*x*x*x*y*y, x*x*x*y*y*y, x*x*y*y*y*y, x*y*y*y*y*y, y*y*y*y*y*y,
		x*x*x*x*x*x*x, x*x*x*x*x*x*y, x*x*x*x*x*y*y, x*x*x*x*y*y*y, x*x*x*y*y*y*y, x*x*y*y*y*y*y, x*y*y*y*y*y*y, y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*y, x*x*x*x*x*x*y*y, x*x*x*x*x*y*y*y, x*x*x*x*y*y*y*y, x*x*x*y*y*y*y*y, x*x*y*y*y*y*y*y, x*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*y*y*y, x*x*x*x*x*y*y*y*y, x*x*x*x*y*y*y*y*y, x*x*x*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y]
X10 = np.c_[np.ones(x.shape[0]),
		x, y, 
		x*x, y*y, x*y,
		x*x*x, x*x*y, x*y*y, y*y*y,
		x*x*x*x, x*x*x*y, x*x*y*y, x*y*y*y, y*y*y*y,
		x*x*x*x*x, x*x*x*x*y, x*x*x*y*y, x*x*y*y*y, x*y*y*y*y, y*y*y*y*y,
		x*x*x*x*x*x, x*x*x*x*x*y, x*x*x*x*y*y, x*x*x*y*y*y, x*x*y*y*y*y, x*y*y*y*y*y, y*y*y*y*y*y,
		x*x*x*x*x*x*x, x*x*x*x*x*x*y, x*x*x*x*x*y*y, x*x*x*x*y*y*y, x*x*x*y*y*y*y, x*x*y*y*y*y*y, x*y*y*y*y*y*y, y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*y, x*x*x*x*x*x*y*y, x*x*x*x*x*y*y*y, x*x*x*x*y*y*y*y, x*x*x*y*y*y*y*y, x*x*y*y*y*y*y*y, x*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*y*y*y, x*x*x*x*x*y*y*y*y, x*x*x*x*y*y*y*y*y, x*x*x*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y]		
X11 = np.c_[np.ones(x.shape[0]),
		x, y, 
		x*x, y*y, x*y,
		x*x*x, x*x*y, x*y*y, y*y*y,
		x*x*x*x, x*x*x*y, x*x*y*y, x*y*y*y, y*y*y*y,
		x*x*x*x*x, x*x*x*x*y, x*x*x*y*y, x*x*y*y*y, x*y*y*y*y, y*y*y*y*y,
		x*x*x*x*x*x, x*x*x*x*x*y, x*x*x*x*y*y, x*x*x*y*y*y, x*x*y*y*y*y, x*y*y*y*y*y, y*y*y*y*y*y,
		x*x*x*x*x*x*x, x*x*x*x*x*x*y, x*x*x*x*x*y*y, x*x*x*x*y*y*y, x*x*x*y*y*y*y, x*x*y*y*y*y*y, x*y*y*y*y*y*y, y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*y, x*x*x*x*x*x*y*y, x*x*x*x*x*y*y*y, x*x*x*x*y*y*y*y, x*x*x*y*y*y*y*y, x*x*y*y*y*y*y*y, x*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*y*y*y, x*x*x*x*x*y*y*y*y, x*x*x*x*y*y*y*y*y, x*x*x*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y]		
X12 = np.c_[np.ones(x.shape[0]),
		x, y, 
		x*x, y*y, x*y,
		x*x*x, x*x*y, x*y*y, y*y*y,
		x*x*x*x, x*x*x*y, x*x*y*y, x*y*y*y, y*y*y*y,
		x*x*x*x*x, x*x*x*x*y, x*x*x*y*y, x*x*y*y*y, x*y*y*y*y, y*y*y*y*y,
		x*x*x*x*x*x, x*x*x*x*x*y, x*x*x*x*y*y, x*x*x*y*y*y, x*x*y*y*y*y, x*y*y*y*y*y, y*y*y*y*y*y,
		x*x*x*x*x*x*x, x*x*x*x*x*x*y, x*x*x*x*x*y*y, x*x*x*x*y*y*y, x*x*x*y*y*y*y, x*x*y*y*y*y*y, x*y*y*y*y*y*y, y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*y, x*x*x*x*x*x*y*y, x*x*x*x*x*y*y*y, x*x*x*x*y*y*y*y, x*x*x*y*y*y*y*y, x*x*y*y*y*y*y*y, x*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*y*y*y, x*x*x*x*x*y*y*y*y, x*x*x*x*y*y*y*y*y, x*x*x*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y]		
X13 = np.c_[np.ones(x.shape[0]),
		x, y, 
		x*x, y*y, x*y,
		x*x*x, x*x*y, x*y*y, y*y*y,
		x*x*x*x, x*x*x*y, x*x*y*y, x*y*y*y, y*y*y*y,
		x*x*x*x*x, x*x*x*x*y, x*x*x*y*y, x*x*y*y*y, x*y*y*y*y, y*y*y*y*y,
		x*x*x*x*x*x, x*x*x*x*x*y, x*x*x*x*y*y, x*x*x*y*y*y, x*x*y*y*y*y, x*y*y*y*y*y, y*y*y*y*y*y,
		x*x*x*x*x*x*x, x*x*x*x*x*x*y, x*x*x*x*x*y*y, x*x*x*x*y*y*y, x*x*x*y*y*y*y, x*x*y*y*y*y*y, x*y*y*y*y*y*y, y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*y, x*x*x*x*x*x*y*y, x*x*x*x*x*y*y*y, x*x*x*x*y*y*y*y, x*x*x*y*y*y*y*y, x*x*y*y*y*y*y*y, x*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*y*y*y, x*x*x*x*x*y*y*y*y, x*x*x*x*y*y*y*y*y, x*x*x*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y]		
X14 = np.c_[np.ones(x.shape[0]),
		x, y, 
		x*x, y*y, x*y,
		x*x*x, x*x*y, x*y*y, y*y*y,
		x*x*x*x, x*x*x*y, x*x*y*y, x*y*y*y, y*y*y*y,
		x*x*x*x*x, x*x*x*x*y, x*x*x*y*y, x*x*y*y*y, x*y*y*y*y, y*y*y*y*y,
		x*x*x*x*x*x, x*x*x*x*x*y, x*x*x*x*y*y, x*x*x*y*y*y, x*x*y*y*y*y, x*y*y*y*y*y, y*y*y*y*y*y,
		x*x*x*x*x*x*x, x*x*x*x*x*x*y, x*x*x*x*x*y*y, x*x*x*x*y*y*y, x*x*x*y*y*y*y, x*x*y*y*y*y*y, x*y*y*y*y*y*y, y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*y, x*x*x*x*x*x*y*y, x*x*x*x*x*y*y*y, x*x*x*x*y*y*y*y, x*x*x*y*y*y*y*y, x*x*y*y*y*y*y*y, x*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*y*y*y, x*x*x*x*x*y*y*y*y, x*x*x*x*y*y*y*y*y, x*x*x*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y]		
X15 = np.c_[np.ones(x.shape[0]),
		x, y, 
		x*x, y*y, x*y,
		x*x*x, x*x*y, x*y*y, y*y*y,
		x*x*x*x, x*x*x*y, x*x*y*y, x*y*y*y, y*y*y*y,
		x*x*x*x*x, x*x*x*x*y, x*x*x*y*y, x*x*y*y*y, x*y*y*y*y, y*y*y*y*y,
		x*x*x*x*x*x, x*x*x*x*x*y, x*x*x*x*y*y, x*x*x*y*y*y, x*x*y*y*y*y, x*y*y*y*y*y, y*y*y*y*y*y,
		x*x*x*x*x*x*x, x*x*x*x*x*x*y, x*x*x*x*x*y*y, x*x*x*x*y*y*y, x*x*x*y*y*y*y, x*x*y*y*y*y*y, x*y*y*y*y*y*y, y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*y, x*x*x*x*x*x*y*y, x*x*x*x*x*y*y*y, x*x*x*x*y*y*y*y, x*x*x*y*y*y*y*y, x*x*y*y*y*y*y*y, x*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*y*y*y, x*x*x*x*x*y*y*y*y, x*x*x*x*y*y*y*y*y, x*x*x*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y*y]		
X16 = np.c_[np.ones(x.shape[0]),
		x, y, 
		x*x, y*y, x*y,
		x*x*x, x*x*y, x*y*y, y*y*y,
		x*x*x*x, x*x*x*y, x*x*y*y, x*y*y*y, y*y*y*y,
		x*x*x*x*x, x*x*x*x*y, x*x*x*y*y, x*x*y*y*y, x*y*y*y*y, y*y*y*y*y,
		x*x*x*x*x*x, x*x*x*x*x*y, x*x*x*x*y*y, x*x*x*y*y*y, x*x*y*y*y*y, x*y*y*y*y*y, y*y*y*y*y*y,
		x*x*x*x*x*x*x, x*x*x*x*x*x*y, x*x*x*x*x*y*y, x*x*x*x*y*y*y, x*x*x*y*y*y*y, x*x*y*y*y*y*y, x*y*y*y*y*y*y, y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*y, x*x*x*x*x*x*y*y, x*x*x*x*x*y*y*y, x*x*x*x*y*y*y*y, x*x*x*y*y*y*y*y, x*x*y*y*y*y*y*y, x*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*y*y*y, x*x*x*x*x*y*y*y*y, x*x*x*x*y*y*y*y*y, x*x*x*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y]		
X17 = np.c_[np.ones(x.shape[0]),
		x, y, 
		x*x, y*y, x*y,
		x*x*x, x*x*y, x*y*y, y*y*y,
		x*x*x*x, x*x*x*y, x*x*y*y, x*y*y*y, y*y*y*y,
		x*x*x*x*x, x*x*x*x*y, x*x*x*y*y, x*x*y*y*y, x*y*y*y*y, y*y*y*y*y,
		x*x*x*x*x*x, x*x*x*x*x*y, x*x*x*x*y*y, x*x*x*y*y*y, x*x*y*y*y*y, x*y*y*y*y*y, y*y*y*y*y*y,
		x*x*x*x*x*x*x, x*x*x*x*x*x*y, x*x*x*x*x*y*y, x*x*x*x*y*y*y, x*x*x*y*y*y*y, x*x*y*y*y*y*y, x*y*y*y*y*y*y, y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*y, x*x*x*x*x*x*y*y, x*x*x*x*x*y*y*y, x*x*x*x*y*y*y*y, x*x*x*y*y*y*y*y, x*x*y*y*y*y*y*y, x*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*y*y*y, x*x*x*x*x*y*y*y*y, x*x*x*x*y*y*y*y*y, x*x*x*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y,		
		x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y]		
X18 = np.c_[np.ones(x.shape[0]),
		x, y, 
		x*x, y*y, x*y,
		x*x*x, x*x*y, x*y*y, y*y*y,
		x*x*x*x, x*x*x*y, x*x*y*y, x*y*y*y, y*y*y*y,
		x*x*x*x*x, x*x*x*x*y, x*x*x*y*y, x*x*y*y*y, x*y*y*y*y, y*y*y*y*y,
		x*x*x*x*x*x, x*x*x*x*x*y, x*x*x*x*y*y, x*x*x*y*y*y, x*x*y*y*y*y, x*y*y*y*y*y, y*y*y*y*y*y,
		x*x*x*x*x*x*x, x*x*x*x*x*x*y, x*x*x*x*x*y*y, x*x*x*x*y*y*y, x*x*x*y*y*y*y, x*x*y*y*y*y*y, x*y*y*y*y*y*y, y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*y, x*x*x*x*x*x*y*y, x*x*x*x*x*y*y*y, x*x*x*x*y*y*y*y, x*x*x*y*y*y*y*y, x*x*y*y*y*y*y*y, x*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*y*y*y, x*x*x*x*x*y*y*y*y, x*x*x*x*y*y*y*y*y, x*x*x*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y,		
		x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y,		
		x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y]		
X19 = np.c_[np.ones(x.shape[0]),
		x, y, 
		x*x, y*y, x*y,
		x*x*x, x*x*y, x*y*y, y*y*y,
		x*x*x*x, x*x*x*y, x*x*y*y, x*y*y*y, y*y*y*y,
		x*x*x*x*x, x*x*x*x*y, x*x*x*y*y, x*x*y*y*y, x*y*y*y*y, y*y*y*y*y,
		x*x*x*x*x*x, x*x*x*x*x*y, x*x*x*x*y*y, x*x*x*y*y*y, x*x*y*y*y*y, x*y*y*y*y*y, y*y*y*y*y*y,
		x*x*x*x*x*x*x, x*x*x*x*x*x*y, x*x*x*x*x*y*y, x*x*x*x*y*y*y, x*x*x*y*y*y*y, x*x*y*y*y*y*y, x*y*y*y*y*y*y, y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*y, x*x*x*x*x*x*y*y, x*x*x*x*x*y*y*y, x*x*x*x*y*y*y*y, x*x*x*y*y*y*y*y, x*x*y*y*y*y*y*y, x*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*y*y*y, x*x*x*x*x*y*y*y*y, x*x*x*x*y*y*y*y*y, x*x*x*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y,		
		x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y,		
		x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y,		
		x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y]		
X20 = np.c_[np.ones(x.shape[0]),
		x, y, 
		x*x, y*y, x*y,
		x*x*x, x*x*y, x*y*y, y*y*y,
		x*x*x*x, x*x*x*y, x*x*y*y, x*y*y*y, y*y*y*y,
		x*x*x*x*x, x*x*x*x*y, x*x*x*y*y, x*x*y*y*y, x*y*y*y*y, y*y*y*y*y,
		x*x*x*x*x*x, x*x*x*x*x*y, x*x*x*x*y*y, x*x*x*y*y*y, x*x*y*y*y*y, x*y*y*y*y*y, y*y*y*y*y*y,
		x*x*x*x*x*x*x, x*x*x*x*x*x*y, x*x*x*x*x*y*y, x*x*x*x*y*y*y, x*x*x*y*y*y*y, x*x*y*y*y*y*y, x*y*y*y*y*y*y, y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*y, x*x*x*x*x*x*y*y, x*x*x*x*x*y*y*y, x*x*x*x*y*y*y*y, x*x*x*y*y*y*y*y, x*x*y*y*y*y*y*y, x*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*y*y*y, x*x*x*x*x*y*y*y*y, x*x*x*x*y*y*y*y*y, x*x*x*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y*y,
		x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y,		
		x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y,		
		x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y,		
		x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y,		
		x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, x*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y, y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y*y]

X_degree_list = [X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, X16, X17, X18, X19, X20]
"""
# Matrix inversion to find beta
beta_OLS = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)
ytilde_OLS = X @ beta
"""
# We must use SVD since matrix inversion on high-dimensional matrices has a 
# higher likelihood of being made impossible due to singularity. It is the 
# case here with a 100*100 design matrix. We do not use ridge regression here 
# even though it would also be suitable to get around singularity.

# Singular value decomposition (SVD) to find beta
X_n = X.copy()
z_n = z.copy()
import numpy as np
# SVD inversion
def SVDinv(A):
    ''' Takes as input a numpy matrix A and returns inv(A) based on singular value decomposition (SVD).
    SVD is numerically more stable than the inversion algorithms provided by
    numpy and scipy.linalg at the cost of being slower.
    '''
    U, s, VT = np.linalg.svd(A)
    D = np.zeros((len(U),len(VT)))
    for i in range(0,len(VT)):
        D[i,i]=s[i]
    UT = np.transpose(U); V = np.transpose(VT); invD = np.linalg.pinv(D) # !!!np.linalg.pinv instead of np.linalg.inv!!!
    return np.matmul(V,np.matmul(invD,UT))

A = np.transpose(X) @ X
C = SVDinv(A)
#print("C", "\n", pd.DataFrame(C))
beta_SVD = C.dot(X.T).dot(np.ravel(FrankeFunction(x, y))) # Finding beta for z without noise
ztilde_SVD = X @ beta_SVD                       # Finding beta for z without noise

#print("beta_SVD", "\n", pd.DataFrame(beta_SVD))
#print("ytilde_SVD", "\n", pd.DataFrame(ytilde_SVD))
print("Mean squared error (own SVD code, no noise): %.2f" % mean_squared_error(FrankeFunction(x, y), ztilde_SVD))
print('Variance score (own SVD code, no noise): %.2f' % r2_score(FrankeFunction(x, y), ztilde_SVD))

beta_SVD = C.dot(X.T).dot(z) # Finding beta for z with noise
ztilde_SVD = X @ beta_SVD                       # Finding beta for z with noise
print("Mean squared error (own SVD code, with noise): %.2f" % mean_squared_error(z, ztilde_SVD))
print('Variance score (own SVD code, with noise): %.2f' % r2_score(z, ztilde_SVD))

def R2(y_data, y_model):
    return 1 - np.sum((y_data - y_model) ** 2) / np.sum((y_data - np.mean(y_data)) ** 2)

def MSE(y_data,y_model):
    n = np.size(y_model)
    return np.sum((y_data-y_model)**2)/n

def beta_SVD(X, z):
  A = np.transpose(X) @ X
  beta_SVD = SVDinv(A).dot(X.T).dot(z)
  #ztilde_SVD = X @ beta_SVD
  return beta_SVD

# OLS without splitting and resampling
def ols_svd_no_resampling(mat_list, starting_degree):
    degree = starting_degree
    degree_list = []
    mse_OLS = []
    r_squared_OLS = []
    for mat in mat_list:
        ztilde = mat @ beta_SVD(mat, z)
        mse = MSE(z, ztilde)
        mse_OLS.append(mse)
        r_squared = R2(z, ztilde)
        r_squared_OLS.append(r_squared)
        degree_list.append(degree)
        degree += 1
    return mse_OLS, r_squared_OLS, degree_list

ols_no_split = ols_svd_no_resampling(X_degree_list, 1)

print("OLS without splitting and resampling")
plt.figure()
plt.plot(ols_no_split[2], ols_no_split[0], label = 'mse OLS')
plt.plot(ols_no_split[2], ols_no_split[1], label = 'r_squared OLS')
plt.xlabel("Model complexity (polynomial order)")
plt.xticks(np.arange(1, len(X_degree_list)+1, 1))
plt.xlabel('Complexity (polynomial degree)')
plt.ylabel('mse')
#plt.yscale('log')
plt.legend()
plt.show()

# Part B
print("Part B")

X_train, X_test, z_train, z_test = train_test_split(X, z, test_size=0.25, random_state=1)

# Finding beta with SVD on split data
def ols_svd(X: np.ndarray, z: np.ndarray) -> np.ndarray:    # Regression, slide 106
    u, s, v = scp.linalg.svd(X)
    return v.T @ scp.linalg.pinv(scp.linalg.diagsvd(s, u.shape[0], v.shape[0])) @ u.T @ z

beta = ols_svd(X_train, z_train)    # Morten's function (Regression, slide 106)

## Cross-validation on OLS regression using KFold only
# Initialize a KFold instance
k = 5
kfold = KFold(n_splits = k)

# Perform the cross-validation to estimate MSE
def ols_svd_ebv_kfold(mat_list, starting_degree, k, shuffle=True):
  X_trainz, X_testz, y_trainz, y_testz = train_test_split(X,z,test_size=1./k)
  array_size=X_trainz.shape
  degree = starting_degree
  degree_list = []
  kfold = KFold(n_splits = k, shuffle=shuffle, random_state=5)
  scores_KFold = np.zeros((len(mat_list), k))
  bias_Kfold = np.zeros((len(mat_list), k))
  variance_Kfold = np.zeros((len(mat_list), k))
  scores_train_KFold = np.zeros((len(mat_list), k))
  #estimated_mse_KFold = []
  #estimated_bias_KFold = []
  #estimated_variance_KFold = []
  #estimated_train_mse_Kfold = []
  i = 0
  for mat in mat_list:
      z_pred_train = np.zeros((array_size[0], k))
      j = 0
      for train_inds, test_inds in kfold.split(mat):
          Xtrain = mat[train_inds]
          ztrain = z[train_inds]
          Xtest = mat[test_inds]
          ztest = z[test_inds]
          z_pred[:, j] = Xtest @ beta_SVD(Xtrain, ztrain)
          z_pred_train[:, j] = Xtrain @ beta_SVD(Xtrain, ztrain)
          j += 1
      scores_KFold[i] = np.mean( np.mean((ztest[:, np.newaxis] - z_pred)**2, axis=1, keepdims=True) )
      bias_Kfold[i] = np.mean( (ztest[:, np.newaxis] - np.mean(z_pred, axis=1, keepdims=True))**2 )
      variance_Kfold[i] = np.mean( np.var(z_pred, axis=1, keepdims=True) )
      scores_train_KFold[i] = np.mean( np.mean((ztrain[:, np.newaxis] - z_pred_train)**2, axis=1, keepdims=True) )
      i += 1
      degree_list.append(degree)
      degree += 1
  estimated_mse_KFold = np.mean(scores_KFold, axis = 1)
  estimated_bias_KFold = np.mean(bias_Kfold, axis = 1)
  estimated_variance_KFold = np.mean(variance_Kfold, axis = 1)
  estimated_train_mse_Kfold = np.mean(scores_train_KFold, axis = 1)
  return estimated_mse_KFold, estimated_bias_KFold, estimated_variance_KFold, degree_list, estimated_train_mse_Kfold

print("OLS with splitting and resampling")
ols_kfold_a = ols_svd_ebv_kfold(X_degree_list, 1, 5)
plt.figure()
plt.plot(ols_kfold_a[3], ols_kfold_a[0], label = 'mse OLS kfold')
plt.plot(ols_kfold_a[3], ols_kfold_a[1], label = 'bias OLS kfold')
plt.plot(ols_kfold_a[3], ols_kfold_a[2], label = 'variance OLS kfold')
#plt.plot(ols_kfold_a[4], ols_kfold_a[3], label = 'r_squared OLS')
plt.xlabel("Model complexity (polynomial order)")
plt.xticks(np.arange(1, len(X_degree_list)+1, 1))
plt.xlabel('Complexity (polynomial degree)')
plt.ylabel('mse')
#plt.yscale('log')
plt.legend()
plt.show()

print("Training and test error for OLS with splitting and resampling")
plt.figure()
plt.plot(ols_kfold_a[3], ols_kfold_a[0], label = 'OLS test kfold')
plt.plot(ols_kfold_a[3], ols_kfold_a[4], label = 'OLS training kfold')
plt.xlabel("Model complexity (polynomial order)")
plt.xticks(np.arange(1, len(X_degree_list)+1, 1))
plt.xlabel('Complexity (polynomial degree)')
plt.ylabel('mse')
#plt.yscale('log')
plt.legend()
plt.show()

# Gathering error, bias and variance for OLS
error_by_complexity_OLS = ols_kfold_a[0]
bias_by_complexity_OLS = ols_kfold_a[1]
variance_by_complexity_OLS = ols_kfold_a[2]

## Cross-validation on Ridge regression using KFold only

# Decide degree on polynomial to fit
poly = PolynomialFeatures(degree = 5)

# Decide which values of lambda to use
nlambdas = 500
lambdas = np.logspace(-12, 5, nlambdas)

# Initialize a KFold instance
k = 5
kfold = KFold(n_splits = k)

# Perform the cross-validation to estimate MSE
scores_KFold = np.zeros((nlambdas, k))

i = 0
for lmb in lambdas:
    ridge = Ridge(alpha = lmb)
    j = 0
    for train_inds, test_inds in kfold.split(X):
        Xtrain = X[train_inds]
        ztrain = z[train_inds]
        
        Xtest = X[test_inds]
        ztest = z[test_inds]
        
        ridge.fit(Xtrain, ztrain)

        zpred = ridge.predict(Xtest)

        scores_KFold[i,j] = np.sum((zpred - ztest)**2)/np.size(zpred)

        j += 1
    i += 1


estimated_mse_KFold = np.mean(scores_KFold, axis = 1)

## Cross-validation using cross_val_score from sklearn along with KFold

# kfold is an instance initialized above as:
# kfold = KFold(n_splits = k)

estimated_mse_sklearn = np.zeros(nlambdas)
i = 0
for lmb in lambdas:
    ridge = Ridge(alpha = lmb)

    estimated_mse_folds = cross_val_score(ridge, X, z, scoring='neg_mean_squared_error', cv=kfold)

    # cross_val_score return an array containing the estimated negative mse for every fold.
    # we have to the the mean of every array in order to get an estimate of the mse of the model
    estimated_mse_sklearn[i] = np.mean(-estimated_mse_folds)

    i += 1

## Plot and compare the slightly different ways to perform cross-validation
plt.figure()
plt.plot(np.log10(lambdas), estimated_mse_sklearn, label = 'cross_val_score')
plt.plot(np.log10(lambdas), estimated_mse_KFold, 'r--', label = 'KFold')
plt.xlabel('log10(lambda)')
plt.ylabel('mse')
plt.legend()
plt.show()

# Part C
print("Part C")

n = 500
n_boostraps = 5
degree = 5

# Hold out some test data that is never used in training.
x_train, x_test, z_train, z_test = train_test_split(X_n, z_n, test_size=0.2)

# Combine x transformation and model into one operation.
# Not neccesary, but convenient.
model = make_pipeline(PolynomialFeatures(degree=degree), LinearRegression(fit_intercept=False))
model = LinearRegression(fit_intercept=False)
# The following (m x n_bootstraps) matrix holds the matrices z_pred
# for each bootstrap iteration.

z_pred = np.empty((z_test.shape[0], n_boostraps))
error_c = []
bias_c = []
variance_c = []
bias_plus_variance_c = []
for i in range(n_boostraps):
    x_, z_ = resample(x_train, z_train)

    # Evaluate the new model on the same test data each time.
    z_pred[:,i] = model.fit(x_, z_).predict(x_test)

#z_test = z_test.reshape(z_test.shape[0],1)
z_test = z_test[:, np.newaxis]
print("x_test, z_test", x_test.shape, z_test.shape)
error = np.mean( np.mean((z_test - z_pred)**2, axis=1, keepdims=True) )
bias = np.mean( (z_test - np.mean(z_pred, axis=1, keepdims=True))**2 )
variance = np.mean( np.var(z_pred, axis=1, keepdims=True) )
   
bias_plus_variance=bias + variance
    #print('Error:', error)
    #print('Bias^2:', bias)
    #print('Var:', variance)
    #print('{} >= {} + {} = {}'.format(error, bias, variance, bias+variance))

print("mean error_c", error)
print("mean bias_c", bias)
print("mean variance_c",variance)
print("mean bias_plus_variance_c", bias + variance)
"""
plt.plot(x[::5, :], z[::5, :], label='f(x)')
plt.scatter(x_test, z_test, label='Data points')
plt.scatter(x_test, np.mean(z_pred, axis=1), label='Pred')
plt.legend()
plt.show()
"""

# Plot the surface. !!!Add predicted z values to Franke function without noise
# Issue: use a matrix of mean corresponding z_pred values among bootstraps?
# Solution: use all training data to avoid mismatches between x and y values
"""
fig = plt.figure()
ax = fig.gca(projection="3d")
surf = ax.plot_surface(x, y, FrankeFunction(x, y), cmap=cm.coolwarm,linewidth=0, 
                       antialiased=False)
surf = ax.plot_surface(x, y, z_pred, cmap=cm.coolwarm,linewidth=0, 
                       antialiased=False)

# Customize the z axis.
ax.set_zlim(-0.10, 1.40)
ax.zaxis.set_major_locator(LinearLocator(10))
ax.zaxis.set_major_formatter(FormatStrFormatter("%.02f"))

# Add a color bar which maps values to colors.
fig.colorbar(surf, shrink=0.5, aspect=5)
plt.show()
"""
"""
# Figure similar to Fig. 2.11 of Hastie, Tibshirani, and Friedman
def Fig_2_11_no_resampling(design_mat, degree):  # Using OLS => problems arising from singularities in design matrices
  X = design_mat
  x_train, x_test, z_train, z_test = train_test_split(design_mat, z, test_size=0.2)
  model = make_pipeline(PolynomialFeatures(degree=degree), LinearRegression(fit_intercept=False))
  z_pred = np.empty((z_test.shape[0]*z_test.shape[1], 1))
  z_train_pred = np.empty((z_train.shape[0]*z_train.shape[1], 1))
  z_pred = model.fit(x_train, z_train).predict(x_test)
  z_train_pred = model.fit(x_train, z_train).predict(x_train)
  error = np.mean( np.mean((z_test - z_pred)**2, axis=1, keepdims=True) )
  bias = np.mean( (z_test - np.mean(z_pred, axis=1, keepdims=True))**2 )
  variance = np.mean( np.var(z_pred, axis=1, keepdims=True) )
  train_error = np.mean( np.mean((z_train - z_train_pred)**2, axis=1, keepdims=True) )
  return error, bias, variance, train_error
"""

def Fig_2_11_no_resampling(design_mat, degree):  # Using SVD
  x_train, x_test, z_train, z_test = train_test_split(design_mat, z, test_size=0.3)
  z_pred = x_test @ beta_SVD(x_train, z_train)
  z_train_pred = x_train @ beta_SVD(x_train, z_train)
  error = np.mean((z_test - z_pred)**2)
  train_error = np.mean((z_train - z_train_pred)**2 )
  #print("z_pred")
  #print(pd.DataFrame(z_pred))
  #print("z_train_pred")
  #print(pd.DataFrame(z_train_pred))
  return error, train_error

def Fig_2_11(design_mat, degree, n_bootstraps):
  x_train, x_test, z_train, z_test = train_test_split(design_mat, z, test_size=0.3)
  z_pred = np.empty((z_test.shape[0], n_boostraps))
  for i in range(n_boostraps):
    x_, z_ = resample(x_train, z_train)
    z_pred[:,i] = x_test @ beta_SVD(x_, z_)
  z_test = z_test[:, np.newaxis]
  error = np.mean( np.mean((z_test - z_pred)**2, axis=1, keepdims=True) )
  bias = np.mean( (z_test - np.mean(z_pred, axis=1, keepdims=True))**2 )
  variance = np.mean( np.var(z_pred, axis=1, keepdims=True) )
  return error,bias,variance 

# No resampling
def generate_error_bias_variance_without_resampling(mat_list, starting_degree):
  degree = starting_degree
  degree_list = []
  Fig_2_11_error_list= []
  Fig_2_11_train_error_list = []
  for mat in mat_list:
    #print(pd.DataFrame(mat))
    #print(degree)
    #print("Start", degree, time.clock())
    Fig_2_11_result = Fig_2_11_no_resampling(mat, degree)
    Fig_2_11_error_list.append(Fig_2_11_result[0])
    Fig_2_11_train_error_list.append(Fig_2_11_result[1])
    degree_list.append(degree)
    degree = degree + 1
    #print(Fig_2_11_result)
    #print("End", time.clock())
  return Fig_2_11_error_list, Fig_2_11_train_error_list, degree_list

# With resampling
def generate_error_bias_variance_with_resampling(mat_list, starting_degree, bootstrap):
  degree = starting_degree
  degree_list = []
  Fig_2_11_error_list= []
  Fig_2_11_bias_list = []
  Fig_2_11_variance_list = []
  for mat in mat_list:
    #print(pd.DataFrame(mat))
    #print(degree)
    #print("Start", degree, time.clock())
    Fig_2_11_result = Fig_2_11(mat, degree, bootstrap)
    Fig_2_11_error_list.append(Fig_2_11_result[0])
    Fig_2_11_bias_list.append(Fig_2_11_result[1])
    Fig_2_11_variance_list.append(Fig_2_11_result[2])
    degree_list.append(degree)
    degree = degree + 1
    #print(Fig_2_11_result)
    #print("End", time.clock())
  return Fig_2_11_error_list, Fig_2_11_bias_list, Fig_2_11_variance_list, degree_list

# Plot error, bias and variance as a function of model complexity
def ebv_by_model_complexity(metrics):
  plt.plot(metrics[3], metrics[0])
  plt.plot(metrics[3], metrics[1])
  plt.plot(metrics[3], metrics[2])
  plt.scatter(metrics[3], metrics[0], label='error')
  plt.scatter(metrics[3], metrics[1], label='bias')
  plt.scatter(metrics[3], metrics[2], label='variance')
  plt.xlabel("Model complexity (polynomial order)")
  plt.xticks(np.arange(1, len(metrics[3])+1, 1))
  #plt.yticks(np.arange(0, 0.2, 0.05))
  #plt.yscale('log')
  plt.legend()
  plt.show()

# Plot training and sample errors as functions of model complexity
def training_vs_test(metrics):
  plt.plot(metrics[2], metrics[0])
  plt.plot(metrics[2], metrics[1])
  plt.scatter(metrics[2], metrics[0], label='Test sample')
  plt.scatter(metrics[2], metrics[1], label='Training sample')
  plt.xlabel("Model complexity (polynomial order)")
  plt.xticks(np.arange(1, len(metrics[2])+1, 1))
  #plt.yticks(np.arange(0, 0.2, 0.05))
  #plt.yscale('log')
  plt.legend()
  plt.show()

ebv_no_resampling = generate_error_bias_variance_without_resampling(X_degree_list, 1)
ebv_resampling = generate_error_bias_variance_with_resampling(X_degree_list, 1, 100)
ebv_by_model_complexity(ebv_resampling)
training_vs_test(ebv_no_resampling)

ebv_no_resampling = generate_error_bias_variance_without_resampling(X_degree_list[0:16], 1)
ebv_resampling = generate_error_bias_variance_with_resampling(X_degree_list[0:16], 1, 100)
ebv_by_model_complexity(ebv_resampling)
training_vs_test(ebv_no_resampling)

ebv_no_resampling = generate_error_bias_variance_without_resampling(X_degree_list[0:6], 1)
ebv_resampling = generate_error_bias_variance_with_resampling(X_degree_list[0:6], 1, 100)
ebv_by_model_complexity(ebv_resampling)
training_vs_test(ebv_no_resampling)

ebv_no_resampling = generate_error_bias_variance_without_resampling(X_degree_list[0:12], 1)
ebv_resampling = generate_error_bias_variance_with_resampling(X_degree_list[0:12], 1, 100)
ebv_by_model_complexity(ebv_resampling)
training_vs_test(ebv_no_resampling)

"""
def model_fit_to_true_function(design_mat, response_mat, degrees, nsamples):
  n_samples = nsamples
  X = mat
  z = response_mat
  
  x = np.sort(np.random.rand(n_samples))
  y = np.sort(np.random.rand(n_samples))
  z = FrankeFunction(x,y) + f_noise_franke(x, y)

  plt.figure(figsize=(14, 5))
  for i in range(len(degrees)):
    ax = plt.subplot(1, len(degrees), i + 1)
    plt.setp(ax, xticks=(), yticks=())

    polynomial_features = PolynomialFeatures(degree=degrees[i],
                                             include_bias=False)
    linear_regression = LinearRegression()
    pipeline = Pipeline([("polynomial_features", polynomial_features),
                         ("linear_regression", linear_regression)])
    pipeline.fit(X, z)

    # Evaluate the models using crossvalidation
    scores = cross_val_score(pipeline, X[:, np.newaxis], y,
                             scoring="neg_mean_squared_error", cv=10)

    X_test = np.linspace(0, 1, 100)
    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label="Model")
    plt.plot(X_test, true_fun(X_test), label="True function")
    plt.scatter(X, y, edgecolor='b', s=20, label="Samples")
    plt.xlabel("x")
    plt.ylabel("y")
    plt.xlim((0, 1))
    plt.ylim((-2, 2))
    plt.legend(loc="best")
    plt.title("Degree {}\nMSE = {:.2e}(+/- {:.2e})".format(
        degrees[i], -scores.mean(), scores.std()))
  plt.show()
"""
# Part D
print("Part D")

##2.k-fold cross-validation
# A seed just to ensure that the random numbers are the same for every run.
# Useful for eventual debugging.
np.random.seed(3155)

# Decide degree on polynomial to fit
poly = PolynomialFeatures(degree = 5)

# Decide which values of lambda to use
nlambdas = 500
lambdas = np.logspace(-12, 5, nlambdas)

# Initialize a KFold instance
k = 5
kfold = KFold(n_splits = k)
scores_KFold = np.zeros((nlambdas, k))
i = 0
for lmb in lambdas:
    ridge = Ridge(alpha = lmb)
    j = 0
    for train_inds, test_inds in kfold.split(X):
        Xtrain = X[train_inds]
        ztrain = z[train_inds]
        Xtest = X[test_inds]
        ztest = z[test_inds]
        ridge.fit(Xtrain, ztrain)
        zpred = ridge.predict(Xtest)
        scores_KFold[i,j] = np.sum((zpred - ztest)**2)/np.size(zpred)
        j += 1
    i += 1

estimated_mse_KFold = np.mean(scores_KFold, axis = 1, keepdims = True)
plt.figure()
plt.plot(np.log10(lambdas), estimated_mse_KFold, 'r--', label = 'KFold')
plt.xlabel('log10(lambda)')
plt.ylabel('mse')
plt.legend()
plt.show()

#print("bias-variance tradeo")
"""
#n = 500
n_boostraps = 100
degree = 5

# Hold out some test data that is never used in training.
x_train, x_test, z_train, z_test = train_test_split(X, z, test_size=0.3)
#print("x_test, z_test", x_test.shape, z_test.shape)
# Combine x transformation and model into one operation.
# Not neccesary, but convenient.
model = make_pipeline(PolynomialFeatures(degree=degree), LinearRegression(fit_intercept=False))

# The following (m x n_bootstraps) matrix holds the matrices z_pred
# for each bootstrap iteration.

z_pred = np.empty((z_test.shape[0], n_boostraps))
error_c = []
bias_c = []
variance_c = []
bias_plus_variance_c = []
for i in range(n_boostraps):
    x_, z_ = resample(x_train, z_train)
    # Evaluate the new model on the same test data each time.
    z_pred = (model.fit(x_, z_).predict(x_test))[:, np.newaxis]
#z_test = z_test.reshape(z_test.shape[0],1)
z_test = z_test[:, np.newaxis]
error = np.mean( np.mean((z_test - z_pred)**2, axis=1, keepdims=True) )
bias = np.mean( (z_test - np.mean(z_pred, axis=1, keepdims=True))**2 )
variance = np.mean( np.var(z_pred, axis=1, keepdims=True) )
print('Error:', error)
print('Bias^2:', bias)
print('Var:', variance)
print('{} >= {} + {} = {}'.format(error, bias, variance, bias+variance))
    
    
#print("mean error_c", np.mean(error_c))
#print("mean bias_c", np.mean(bias_c))
#print("mean variance_c", np.mean(variance_c))
#print("mean bias_plus_variance_c", np.mean(bias + variance))

###CV
print ("CV")
#Bootstrap part and initializations 
error_d = []
bias_d =[]
variance_d =[]

n = 10
n_boostraps = 10

#Polynomial degree
degrees = np.arange(1,16)

#Bootstrap part
for degree in degrees:
    x_train, x_test, z_train, z_test = train_test_split(X, z, test_size=0.3)
    model = make_pipeline(PolynomialFeatures(degree=degree), LinearRegression(fit_intercept=False))
    z_pred = np.empty((z_test.shape[0], n_boostraps))
    for i in range(n_boostraps):
        x_, z_ = resample(x_train, z_train)
        # Evaluate the new model on the same test data each time.
        z_pred = (x_test @ beta_SVD(x_, z_))[:, np.newaxis]
    error = np.mean( np.mean((z_test - z_pred)**2, axis=1, keepdims=True) )
    #z_test = z_test.reshape(z_test.shape[0],1)
    z_test = z_test[:, np.newaxis]
    bias = np.mean( (z_test - np.mean(z_pred, axis=1, keepdims=True))**2 )
    variance = np.mean( np.var(z_pred, axis=1, keepdims=True) )
    error_d.append(error)
    bias_d.append(bias)
    variance_d.append(variance)

max_pd = 12 #max polynomial degree to plot to
plt.figure()
plt.plot(degrees[:max_pd],error_d[:max_pd],'k',label='MSE')
plt.plot(degrees[:max_pd],bias_d[:max_pd],'b',label='Bias^2')
plt.plot(degrees[:max_pd],variance_d[:max_pd],'y',label='Var')
summ=np.zeros(len(variance_d))
for i in range(len(error_d)):
    summ[i]=variance_d[i]+bias_d[i]
plt.plot(degrees[:max_pd],summ[:max_pd],'ro',label='sum')
plt.xlabel('Polynomial degree')
plt.ylabel('MSE Bootstrap')
plt.legend()
plt.show()

# Cross-validation using Scikit-Learn's KFold function
#initiate stuff again in case data was changed earlier
N=10
k=5

degrees = np.arange(1,16)

kfold = KFold(n_splits = k, shuffle=True, random_state=5)

#Two clumsy lines to get the size of y_pred array right
X_trainz, X_testz, z_trainz, z_testz = train_test_split(X, z, test_size=1./k)
array_size=len(z_testz)

error_d = []
bias_d =[]
variance_d =[]

for deg in degrees:
    z_pred = np.empty((array_size, k))
    j=0
    model = make_pipeline(PolynomialFeatures(degree=deg),LinearRegression(fit_intercept=False))
    for train_inds,test_inds in kfold.split(x):
        Xtrain = X[train_inds]
        ztrain = z[train_inds]
        Xtest = X[test_inds]
        ztest = z[test_inds]
        z_pred = (x_test @ beta_SVD(Xtrain, ztrain))[:, np.newaxis]
        j+=1
    error = np.mean( np.mean((ztest - z_pred)**2, axis=1, keepdims=True) )
    bias = np.mean( (ztest - np.mean(z_pred, axis=1, keepdims=True))**2 )
    variance = np.mean( np.var(z_pred, axis=1, keepdims=True) )
    error_d.append(error)
    bias_d.append(bias)
    variance_d.append(variance)

max_pd = 12 #max polynomial degree to plot to
plt.figure()
plt.plot(degrees[:max_pd],error_d[:max_pd],'k',label='MSE')
plt.plot(degrees[:max_pd],bias_d[:max_pd],'b',label='Bias^2')
plt.plot(degrees[:max_pd],variance_d[:max_pd],'y',label='Var')
summ=np.zeros(len(variance_d))
for i in range(len(error_d)):
    summ[i]=variance_d[i]+bias_d[i]
plt.plot(degrees[:max_pd],summ[:max_pd],'ro',label='sum')

plt.xlabel('Polynomial degree')
plt.ylabel('MSE CV')
plt.legend()
plt.show()
"""
### sklearn ridge vs own code
def optimal_lambda_ridge(MSE_list, lambdas):
    mse = np.amin(MSE_list)
    highest_lambda_at_lowest_MSE = lambdas[np.where(MSE_list == np.amin(MSE_list))[0][-1]]
    return mse, highest_lambda_at_lowest_MSE

#Alternate functions for ridge regression

def ridge_regression_sklearn(design_mat, lambdas, k, shuffle=True):
  kfold = KFold(n_splits = k, shuffle=shuffle, random_state=5)
  scores_KFold = np.zeros((nlambdas, k))
  i = 0
  for lmb in lambdas:
      ridge = Ridge(alpha = lmb,fit_intercept=False)
      j = 0
      for train_inds, test_inds in kfold.split(design_mat):
          Xtrain = design_mat[train_inds]
          ztrain = z[train_inds]
          Xtest = design_mat[test_inds]
          ztest = z[test_inds]
          ridge.fit(Xtrain, ztrain)
          zpred = ridge.predict(Xtest)
          scores_KFold[i,j] = np.sum((zpred - ztest)**2)/np.size(zpred)
          j += 1
      i += 1
  estimated_mse_KFold_sklearn = np.mean(scores_KFold, axis = 1)
  return estimated_mse_KFold_sklearn

def ridge_regression_own(design_mat, lambdas, k, shuffle=True):
  kfold = KFold(n_splits = k, shuffle=shuffle, random_state=5)
  scores_KFold = np.zeros((nlambdas, k))
  bias_Kfold = np.zeros((nlambdas, k))
  variance_Kfold = np.zeros((nlambdas, k))
  mse_train_Kfold = np.zeros((nlambdas, k))
  i = 0
  for lmb in lambdas:
      j = 0
      for train_inds, test_inds in kfold.split(design_mat):
          Xtrain = design_mat[train_inds]
          ztrain = z[train_inds]
          Xtest = design_mat[test_inds]
          ztest = z[test_inds]
          #print("ztest", ztest.shape)
          z_pred = Xtest @ BetaRidge(Xtrain, ztrain, lmb)
          #print("z_pred", z_pred.shape)
          z_pred_train = Xtrain @ BetaRidge(Xtrain, ztrain, lmb)
          scores_KFold[i,j] = np.sum( (z_pred - ztest)**2 )/np.size(z_pred)
          bias_Kfold[i,j] = np.sum( (ztest - np.mean(z_pred))**2 )/np.size(z_pred)
          variance_Kfold[i,j] = np.sum( np.var(z_pred) )/np.size(z_pred)
          mse_train_Kfold[i,j] = np.sum( (z_pred_train - ztrain)**2 )/np.size(z_pred_train)
          j += 1
      i += 1
  estimated_mse_KFold_own = np.mean(scores_KFold, axis = 1)
  estimated_bias_Kfold_own = np.mean(bias_Kfold, axis = 1)
  estimated_variance_Kfold_own = np.mean(variance_Kfold, axis = 1)
  estimated_mse_train_Kfold_own = np.mean(mse_train_Kfold, axis = 1)
  return estimated_mse_KFold_own, estimated_bias_Kfold_own, estimated_variance_Kfold_own, estimated_mse_train_Kfold_own

#Alternate functions for ridge regression
"""
def ridge_regression_sklearn(design_mat, lambdas, k, shuffle=True):
  kfold = KFold(n_splits = k, shuffle=shuffle, random_state=5)
  scores_KFold = np.zeros((nlambdas, k))
  i = 0
  for lmb in lambdas:
      ridge = Ridge(alpha = lmb,fit_intercept=False)
      j = 0
      for train_inds, test_inds in kfold.split(design_mat):
          Xtrain = design_mat[train_inds]
          ztrain = z[train_inds]
          Xtest = design_mat[test_inds]
          ztest = z[test_inds]
          ridge.fit(Xtrain, ztrain)
          z_pred[:, j] = ridge.predict(Xtest)
          j += 1
      scores_KFold[i] = np.mean( np.mean((ztest[:, np.newaxis] - z_pred)**2, axis=1, keepdims=True) )
      i += 1
  estimated_mse_KFold_sklearn = np.mean(scores_KFold, axis = 1)
  return estimated_mse_KFold_sklearn

def ridge_regression_own(design_mat, lambdas, k, shuffle=True):
  X_trainz, X_testz, y_trainz, y_testz = train_test_split(x,y,test_size=1./k)
  array_size=X_trainz.shape
  kfold = KFold(n_splits = k, shuffle=shuffle, random_state=5)
  scores_KFold = np.zeros((nlambdas, k))
  bias_Kfold = np.zeros((nlambdas, k))
  variance_Kfold = np.zeros((nlambdas, k))
  mse_train_Kfold = np.zeros((nlambdas, k))
  i = 0
  for lmb in lambdas:
      z_pred_train = np.zeros((array_size[0], k))
      #print("z_pred_train.shape", z_pred_train.shape)
      j = 0
      for train_inds, test_inds in kfold.split(design_mat):
          Xtrain = design_mat[train_inds]
          ztrain = z[train_inds]
          Xtest = design_mat[test_inds]
          ztest = z[test_inds]
          z_pred[:, j] = Xtest @ BetaRidge(Xtrain, ztrain, lmb)
          z_pred_train[:, j] = Xtrain @ BetaRidge(Xtrain, ztrain, lmb)
          j += 1
      scores_KFold[i] = np.mean( np.mean((ztest[:, np.newaxis] - z_pred)**2, axis=1, keepdims=True) )
      bias_Kfold[i] = np.mean( (ztest[:, np.newaxis] - np.mean(z_pred, axis=1, keepdims=True))**2 )
      variance_Kfold[i] = np.mean( np.var(z_pred, axis=1, keepdims=True) )
      mse_train_Kfold[i] = np.mean( np.mean((ztrain[:, np.newaxis] - z_pred_train)**2, axis=1, keepdims=True) )
      i += 1
  estimated_mse_KFold_own = np.mean(scores_KFold, axis = 1, keepdims=True)
  estimated_bias_Kfold_own = np.mean(bias_Kfold, axis = 1, keepdims=True)
  estimated_variance_Kfold_own = np.mean(variance_Kfold, axis = 1, keepdims=True)
  estimated_mse_train_Kfold_own = np.mean(mse_train_Kfold, axis = 1, keepdims=True)
  return estimated_mse_KFold_own, estimated_bias_Kfold_own, estimated_variance_Kfold_own, estimated_mse_train_Kfold_own
"""
def BetaRidge (X, y, lambda_value):
  beta_ridge = np.linalg.pinv(X.T.dot(X) + lambda_value*np.identity(np.shape(X.T.dot(X))[1])).dot(X.T).dot(y)
  #ytilde_ridge = X @ beta_ridge
  return beta_ridge

def ridge_regression_bootstrap(design_mat, n_bootstraps, lambdas):
  error_e = []
  bias_e = []
  variance_e = []
  for lmb in lambdas:    
    x_train, x_test, z_train, z_test = train_test_split(design_mat, z, test_size=0.2)
    z_pred = np.empty((z_test.shape[0], n_boostraps))
    for i in range(n_boostraps):
      x_, z_ = resample(x_train, z_train)
      z_pred[:,i] = x_test @ BetaRidge(x_, z_, lmb)
    z_test = z_test[:, np.newaxis]
    error = np.mean( np.mean((z_test - z_pred)**2, axis=1, keepdims=True) )
    error_e.append(error)
    bias = np.mean( (z_test - np.mean(z_pred, axis=1, keepdims=True))**2 )
    bias_e.append(bias)
    variance = np.mean( np.var(z_pred, axis=1, keepdims=True) )
    variance_e.append(variance)
  return error_e, bias_e, variance_e

# Ridge regression with scikit-learn
print("sklearn")
nlambdas = 100
lambdas = np.logspace(-12, 5, nlambdas)

estimated_mse_KFold_sklearn = ridge_regression_sklearn(X, lambdas, 5, shuffle=True)
print("MSE (sklearn)", optimal_lambda_ridge(estimated_mse_KFold_sklearn, lambdas)[0], "\n",
      "Highest lambda value at lowest MSE (sklearn)=", optimal_lambda_ridge(estimated_mse_KFold_sklearn, lambdas)[1])

# Ridge regression with own code
print("own code")
estimated_mse_KFold_own = ridge_regression_own(X, lambdas, 5, shuffle=True)[0]
print("MSE (own code)", optimal_lambda_ridge(estimated_mse_KFold_own, lambdas)[0], "\n",
      "Highest lambda value at lowest MSE (own code)=", optimal_lambda_ridge(estimated_mse_KFold_own, lambdas)[1])
#
plt.figure()
plt.plot(np.log10(lambdas), estimated_mse_KFold_own, 'r--', label = 'KFold own code')
plt.plot(np.log10(lambdas), estimated_mse_KFold_sklearn, 'g:', label = 'KFold sklearn')
plt.xlabel('log10(lambda)')
plt.ylabel('mse')
plt.legend()
plt.show()

# Ridge regression with bootstrap
print("bootstrap")
rrb = ridge_regression_bootstrap(X, 100, lambdas)
print("MSE (bootstrap)", optimal_lambda_ridge(rrb[0], lambdas)[0], "\n",
      "Highest lambda value at lowest MSE (bootstrap)=", optimal_lambda_ridge(rrb[0], lambdas)[1])

plt.figure()
plt.plot(np.log10(lambdas), estimated_mse_KFold_own, 'b-', label = 'KFold')
plt.plot(np.log10(lambdas), rrb[0], 'r:', label = 'Bootstrap')
plt.xlabel('log10(lambda)')
plt.ylabel('mse')
plt.legend()
plt.show()

def generate_error_bias_variance_with_bootstrap_ridge(mat_list, starting_degree, bootstrap, lambdas):
  degree = starting_degree
  degree_list = []
  Fig_2_11_error_list= []
  Fig_2_11_bias_list = []
  Fig_2_11_variance_list = []
  optimal_lambda_list = []
  for mat in mat_list:
    Fig_2_11_result = ridge_regression_bootstrap(mat, bootstrap, lambdas)
    MSE = optimal_lambda_ridge(Fig_2_11_result[0], lambdas)[0]
    bias = optimal_lambda_ridge(Fig_2_11_result[1], lambdas)[0]
    variance = optimal_lambda_ridge(Fig_2_11_result[2], lambdas)[0]
    optimal_lambda = optimal_lambda_ridge(Fig_2_11_result[0], lambdas)[1]
    Fig_2_11_error_list.append(MSE)
    Fig_2_11_bias_list.append(bias)
    Fig_2_11_variance_list.append(variance)
    optimal_lambda_list.append(optimal_lambda)
    degree_list.append(degree)
    degree = degree + 1
  return Fig_2_11_error_list, Fig_2_11_bias_list, Fig_2_11_variance_list, degree_list, optimal_lambda_list

def generate_error_bias_variance_with_kfold_ridge(mat_list, starting_degree, k, lambdas):
  degree = starting_degree
  degree_list = []
  Fig_2_11_error_list= []
  Fig_2_11_bias_list = []
  Fig_2_11_variance_list = []
  Fig_2_11_train_error_list = []
  optimal_lambda_list = []
  for mat in mat_list:
    Fig_2_11_result = ridge_regression_own(mat, lambdas, k, shuffle=True)
    MSE = optimal_lambda_ridge(Fig_2_11_result[0], lambdas)[0]
    bias = optimal_lambda_ridge(Fig_2_11_result[1], lambdas)[0]
    variance = optimal_lambda_ridge(Fig_2_11_result[2], lambdas)[0]
    train_mse = optimal_lambda_ridge(Fig_2_11_result[3], lambdas)[0]
    optimal_lambda = optimal_lambda_ridge(Fig_2_11_result[0], lambdas)[1]
    Fig_2_11_error_list.append(MSE)
    Fig_2_11_bias_list.append(bias)
    Fig_2_11_variance_list.append(variance)
    optimal_lambda_list.append(optimal_lambda)
    Fig_2_11_train_error_list.append(train_mse)
    degree_list.append(degree)
    degree = degree + 1
  return Fig_2_11_error_list, Fig_2_11_bias_list, Fig_2_11_variance_list, degree_list, optimal_lambda_list, Fig_2_11_train_error_list

def ebv_by_model_complexity_ridge(metrics):
  plt.plot(metrics[3], metrics[0], label='error')
  plt.plot(metrics[3], metrics[1], label='bias')
  plt.plot(metrics[3], metrics[2], label='variance')
  plt.xlabel("Model complexity (polynomial order)")
  plt.xticks(np.arange(1, len(metrics[3])+1, 1))
  plt.yscale('log')
  plt.legend()
  plt.show()
  
def training_vs_test_ridge(metrics):
  plt.plot(metrics[3], metrics[0], label='MSE Test sample')
  plt.plot(metrics[3], metrics[5], label='MSE Training sample')
  plt.xlabel("Model complexity (polynomial order)")
  plt.xticks(np.arange(1, len(metrics[3])+1, 1))
  plt.yscale('log')
  plt.legend()
  plt.show()

# Compare mse, bias and variance for ridge regression with bootstrap and k-fold  
ebv_ridge = generate_error_bias_variance_with_bootstrap_ridge(X_degree_list, 1, 100, lambdas)
ebv_ridge_kfold = generate_error_bias_variance_with_kfold_ridge(X_degree_list, 1, 5, lambdas)

# Gathering error, bias and variance at optimal lambda values by degree for ridge regression
degree_list_ridge = ebv_ridge[3]
print("bootstrap")
ebv_by_model_complexity_ridge(ebv_ridge)
print("kfold")
ebv_by_model_complexity_ridge(ebv_ridge_kfold)
error_by_complexity_ridge_bootstrap = ebv_ridge[0]
error_by_complexity_ridge_kfold = ebv_ridge_kfold[0]
bias_by_complexity_ridge_bootstrap = ebv_ridge[1]
bias_by_complexity_ridge_kfold = ebv_ridge_kfold[1]
variance_by_complexity_ridge_bootstrap = ebv_ridge[2]
variance_by_complexity_ridge_kfold = ebv_ridge_kfold[2]
lambda_by_complexity_ridge_bootstrap = ebv_ridge[4]
lambda_by_complexity_ridge_kfold = ebv_ridge_kfold[4]

# Plot optimal values of lambda by complexity for ridge regression with bootstrap and k-fold
plt.plot(degree_list_ridge, lambda_by_complexity_ridge_bootstrap, label='lambda bootstrap')
plt.plot(degree_list_ridge, lambda_by_complexity_ridge_kfold, label='lambda k-fold')
plt.xticks(np.arange(1, len(degree_list_ridge)+1, 1))
plt.yscale('log')
plt.legend()
plt.show()

#Plot training and test errors for ridge regression with k-fold cross-validation
training_vs_test_ridge(ebv_ridge_kfold)

# Part E
print("Part E")

#Alternate functions for lasso regression
"""
def lasso_regression(design_mat, lambdas, k, shuffle=True):
  kfold = KFold(n_splits = k, shuffle=shuffle, random_state=5)
  scores_KFold = np.zeros((nlambdas, k))
  bias_Kfold = np.zeros((nlambdas, k))
  variance_Kfold = np.zeros((nlambdas, k))
  mse_train_Kfold = np.zeros((nlambdas, k))
  i = 0
  for lmb in lambdas:
      lasso = Lasso(alpha = lmb, max_iter = 1000, tol = 0.0001, fit_intercept=False)
      j = 0
      for train_inds, test_inds in kfold.split(design_mat):
          Xtrain = design_mat[train_inds]
          ztrain = z[train_inds]
          Xtest = design_mat[test_inds]
          ztest = z[test_inds]
          lasso.fit(Xtrain, ztrain)
          betaLasso = lasso.coef_
          zpred = Xtest @ betaLasso
          z_pred_train = Xtrain @ betaLasso
          scores_KFold[i,j] = np.sum( (zpred - ztest)**2 )/np.size(zpred)
          bias_Kfold[i,j] = np.sum( (ztest - np.mean(zpred))**2 )/np.size(zpred)
          variance_Kfold[i,j] = np.sum( np.var(zpred) )/np.size(zpred)
          mse_train_Kfold[i,j] = np.sum( (z_pred_train - ztrain)**2 )/np.size(z_pred_train)
          j += 1
      i += 1
  estimated_mse_KFold_lasso = np.mean(scores_KFold, axis = 1)
  estimated_bias_Kfold_lasso = np.mean(bias_Kfold, axis = 1)
  estimated_variance_Kfold_lasso = np.mean(variance_Kfold, axis = 1)
  estimated_mse_train_Kfold_lasso = np.mean(mse_train_Kfold, axis = 1)
  return estimated_mse_KFold_lasso, estimated_bias_Kfold_lasso, estimated_variance_Kfold_lasso, estimated_mse_train_Kfold_lasso

def generate_error_bias_variance_with_kfold_lasso(mat_list, starting_degree, k, lambdas):
  degree = starting_degree
  degree_list = []
  Fig_2_11_error_list= []
  Fig_2_11_bias_list = []
  Fig_2_11_variance_list = []
  Fig_2_11_train_error_list = []
  optimal_lambda_list = []
  for mat in mat_list:
    Fig_2_11_result = lasso_regression(mat, lambdas, k, shuffle=True)
    MSE = optimal_lambda_ridge(Fig_2_11_result[0], lambdas)[0]
    bias = optimal_lambda_ridge(Fig_2_11_result[1], lambdas)[0]
    variance = optimal_lambda_ridge(Fig_2_11_result[2], lambdas)[0]
    train_mse = optimal_lambda_ridge(Fig_2_11_result[3], lambdas)[0]
    optimal_lambda = optimal_lambda_ridge(Fig_2_11_result[0], lambdas)[1]
    Fig_2_11_error_list.append(MSE)
    Fig_2_11_bias_list.append(bias)
    Fig_2_11_variance_list.append(variance)
    optimal_lambda_list.append(optimal_lambda)
    Fig_2_11_train_error_list.append(train_mse)
    degree_list.append(degree)
    degree = degree + 1
  return Fig_2_11_error_list, Fig_2_11_bias_list, Fig_2_11_variance_list, degree_list, optimal_lambda_list, Fig_2_11_train_error_list
"""
"""
def lasso_regression(design_mat, lambdas, k, shuffle=True):
  X_trainz, X_testz, y_trainz, y_testz = train_test_split(x,y,test_size=1./k)
  array_size=X_trainz.shape
  kfold = KFold(n_splits = k, shuffle=shuffle, random_state=5)
  scores_KFold = np.zeros((nlambdas, k))
  bias_Kfold = np.zeros((nlambdas, k))
  variance_Kfold = np.zeros((nlambdas, k))
  mse_train_Kfold = np.zeros((nlambdas, k))
  i = 0
  for lmb in lambdas:
      lasso = Lasso(alpha = lmb, max_iter = 1000, tol = 0.0001, fit_intercept=False)
      z_pred_train = np.zeros((array_size[0], k))
      #z_pred = np.zeros((X_testz.shape[0], k))
      j = 0
      for train_inds, test_inds in kfold.split(design_mat):
          Xtrain = design_mat[train_inds]
          ztrain = z[train_inds]
          Xtest = design_mat[test_inds]
          ztest = z[test_inds]
          lasso.fit(Xtrain, ztrain)
          betaLasso = lasso.coef_
          z_pred[:, j] = Xtest @ betaLasso
          z_pred_train[:, j] = Xtrain @ betaLasso
          j += 1
      scores_KFold[i] = np.mean( np.mean((ztest[:, np.newaxis] - z_pred)**2, axis=1, keepdims=True) )
      bias_Kfold[i] = np.mean( (ztest[:, np.newaxis] - np.mean(z_pred, axis=1, keepdims=True))**2 )
      variance_Kfold[i] = np.mean( np.var(z_pred, axis=1, keepdims=True) )
      mse_train_Kfold[i] = np.mean( np.mean((ztrain[:, np.newaxis] - z_pred_train)**2, axis=1, keepdims=True) )
      i += 1
  estimated_mse_KFold_lasso = np.mean(scores_KFold, axis = 1, keepdims=True)
  estimated_bias_Kfold_lasso = np.mean(bias_Kfold, axis = 1, keepdims=True)
  estimated_variance_Kfold_lasso = np.mean(variance_Kfold, axis = 1, keepdims=True)
  estimated_mse_train_Kfold_lasso = np.mean(mse_train_Kfold, axis = 1, keepdims=True)
  return estimated_mse_KFold_lasso, estimated_bias_Kfold_lasso, estimated_variance_Kfold_lasso, estimated_mse_train_Kfold_lasso

"""
def lasso_regression(design_mat, lambdas, k, shuffle=True):
  kfold = KFold(n_splits = k, shuffle=shuffle, random_state=5)
  scores_KFold = np.zeros((nlambdas, k))
  bias_Kfold = np.zeros((nlambdas, k))
  variance_Kfold = np.zeros((nlambdas, k))
  mse_train_Kfold = np.zeros((nlambdas, k))
  i = 0
  for lmb in lambdas:
      lasso = Lasso(alpha = lmb, max_iter = 1000, tol = 0.0001, fit_intercept=False)
      j = 0
      for train_inds, test_inds in kfold.split(design_mat):
          Xtrain = design_mat[train_inds]
          ztrain = z[train_inds]
          Xtest = design_mat[test_inds]
          ztest = z[test_inds]
          lasso.fit(Xtrain, ztrain)
          betaLasso = lasso.coef_
          zpred = Xtest @ betaLasso
          z_pred_train = Xtrain @ betaLasso
          scores_KFold[i,j] = np.sum( (zpred - ztest)**2 )/np.size(zpred)
          bias_Kfold[i,j] = np.sum( (ztest - np.mean(zpred))**2 )/np.size(zpred)
          variance_Kfold[i,j] = np.sum( np.var(zpred) )/np.size(zpred)
          mse_train_Kfold[i,j] = np.sum( (z_pred_train - ztrain)**2 )/np.size(z_pred_train)
          j += 1
      i += 1
  estimated_mse_KFold_lasso = np.mean(scores_KFold, axis = 1)
  estimated_bias_Kfold_lasso = np.mean(bias_Kfold, axis = 1)
  estimated_variance_Kfold_lasso = np.mean(variance_Kfold, axis = 1)
  estimated_mse_train_Kfold_lasso = np.mean(mse_train_Kfold, axis = 1)
  return estimated_mse_KFold_lasso, estimated_bias_Kfold_lasso, estimated_variance_Kfold_lasso, estimated_mse_train_Kfold_lasso

def generate_error_bias_variance_with_kfold_lasso(mat_list, starting_degree, k, lambdas):
  degree = starting_degree
  degree_list = []
  Fig_2_11_error_list= []
  Fig_2_11_bias_list = []
  Fig_2_11_variance_list = []
  Fig_2_11_train_error_list = []
  optimal_lambda_list = []
  for mat in mat_list:
    Fig_2_11_result = lasso_regression(mat, lambdas, k, shuffle=True)
    MSE = optimal_lambda_ridge(Fig_2_11_result[0], lambdas)[0]
    bias = optimal_lambda_ridge(Fig_2_11_result[1], lambdas)[0]
    variance = optimal_lambda_ridge(Fig_2_11_result[2], lambdas)[0]
    train_mse = optimal_lambda_ridge(Fig_2_11_result[3], lambdas)[0]
    optimal_lambda = optimal_lambda_ridge(Fig_2_11_result[0], lambdas)[1]
    Fig_2_11_error_list.append(MSE)
    Fig_2_11_bias_list.append(bias)
    Fig_2_11_variance_list.append(variance)
    optimal_lambda_list.append(optimal_lambda)
    Fig_2_11_train_error_list.append(train_mse)
    degree_list.append(degree)
    degree = degree + 1
  return Fig_2_11_error_list, Fig_2_11_bias_list, Fig_2_11_variance_list, degree_list, optimal_lambda_list, Fig_2_11_train_error_list

nlambdas = 100
lambdas = np.logspace(-12, 5, nlambdas)

lasso_degree_5_test_mse = lasso_regression(X, lambdas, 5, shuffle=True)[0]
lasso_degree_5_train_mse = lasso_regression(X, lambdas, 5, shuffle=True)[3]

print("MSE (Lasso)", optimal_lambda_ridge(lasso_degree_5_test_mse, lambdas)[0], "\n",
      "Highest lambda value at lowest MSE (Lasso)=", optimal_lambda_ridge(lasso_degree_5_test_mse, lambdas)[1])

#Compare training and test error for ridge and lasso with k-fold CV
estimated_mse_KFold_own_train = ridge_regression_own(X, lambdas, 5, shuffle=True)[3]
plt.figure()
plt.plot(np.log10(lambdas), estimated_mse_KFold_own, 'b--', label = 'Ridge kfold test')
plt.plot(np.log10(lambdas), estimated_mse_KFold_own_train, 'm:', label = 'Ridge kfold train')
plt.plot(np.log10(lambdas), lasso_degree_5_test_mse, 'r--', label = 'Lasso train')
plt.plot(np.log10(lambdas), lasso_degree_5_train_mse, 'g:', label = 'Lasso test')
plt.xlabel('log10(lambda)')
plt.ylabel('mse')
plt.yscale('log')
plt.legend()
plt.show()

# Examine error, bias and variance by complexity for lasso regression
ebv_lasso = generate_error_bias_variance_with_kfold_lasso(X_degree_list, 1, 5, lambdas)
training_vs_test_ridge(ebv_lasso)
ebv_by_model_complexity_ridge(ebv_lasso)

# Gathering error, bias and variance at optimal lambda values by degree for ridge regression
error_by_complexity_lasso = ebv_lasso[0]
bias_by_complexity_lasso = ebv_lasso[1]
variance_by_complexity_lasso = ebv_lasso[2]
lambda_by_complexity_lasso = ebv_lasso[4]

# Generate table for error, bias, variance and optimal lambda by complexity for OLS, ridge and lasso regression
comparison_list = np.zeros((20, 12))
comparison_list[:,0]  = degree_list_ridge
comparison_list[:,1]  = error_by_complexity_OLS
comparison_list[:,2]  = bias_by_complexity_OLS
comparison_list[:,3]  = variance_by_complexity_OLS
comparison_list[:,4]  = error_by_complexity_ridge_kfold
comparison_list[:,5]  = bias_by_complexity_ridge_kfold
comparison_list[:,6]  = variance_by_complexity_ridge_kfold
comparison_list[:,7]  = lambda_by_complexity_ridge_kfold
comparison_list[:,8]  = error_by_complexity_lasso
comparison_list[:,9]  = bias_by_complexity_lasso
comparison_list[:,10]  = variance_by_complexity_lasso
comparison_list[:,11]  = lambda_by_complexity_lasso
comparison_dataframe = pd.DataFrame(comparison_list)

comparison_dataframe.columns = [
          'Degree', 'MSE OLS', 'Bias OLS', 'Variance OLS',
          'MSE Ridge', 'Bias Ridge', 'Variance Ridge', 'lambda Ridge',
          'MSE Lasso', 'Bias Lasso', 'Variance Lasso', 'lambda Lasso']
print(comparison_dataframe)

